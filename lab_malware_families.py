#!/usr/bin/env python
# coding: utf-8

# # Laboratorio #4 - Familias de Malware

# # Parte 1

# In[1]:


import os
import pefile
import pandas as pd
import hashlib
import re
from datetime import datetime, timezone
import subprocess


# ## Creación del dataset

# In[2]:


MALWARE_DIR = "./MALWR"
malware_files = [os.path.join(MALWARE_DIR, f) for f in os.listdir(MALWARE_DIR) if os.path.isfile(os.path.join(MALWARE_DIR, f))]

print(f"🔍 Se encontraron {len(malware_files)} archivos de malware en el directorio.")


# In[3]:


# 📌 Función para calcular SHA-256
def get_sha256(file_path):
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

# 📌 Función para extraer timestamps
def convert_timestamp(timestamp):
    try:
        return datetime.fromtimestamp(timestamp, timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
    except (OSError, OverflowError, ValueError):
        return "Invalid Timestamp"

# 📌 API sospechosas
suspicious_api_calls = {
    "process hollowing": {
        "CreateProcess", "GetModuleHandle", "GetProcAddress", "VirtualAllocEx", "WriteProcessMemory", "SetThreadContext", "ResumeThread"
    },
    "create remote thread": {
        "OpenProcess", "GetModuleHandle", "GetProcAddress", "VirtualAllocEx", "WriteProcessMemory", "CreateRemoteThread"
    },
    "enumerating processes": {
        "CreateToolhelp32Snapshot", "Process32First", "Process32Next", "WTSEnumerateProcesses"
    },
    "drop file from PE resource": {
        "GetModuleHandle", "FindResource", "LoadResource", "CreateFileA"
    },
    "IAT hooking": {
        "GetModuleHandle", "strcmp", "VirtualProtect"
    },
    "delete itself": {
        "GetModuleFileName", "ExitProcess", "DeleteFile"
    },
    "download and execute PE file": {
        "URLDownloadToFile", "ShellExecute"
    },
    "bind TCP port": {
        "WSAStartup", "socket"
    },
    "capture network traffic": {
        "socket", "bind", "WSAIoctl", "recvfrom"
    }
}

# 📌 Función para extraer importaciones
def extract_imports(pe):
    imported_functions = []
    imported_dlls = []
    detected_suspicious_apis = set()

    if hasattr(pe, "DIRECTORY_ENTRY_IMPORT"):
        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            dll_name = entry.dll.decode(errors="ignore")
            imported_dlls.append(dll_name)

            for function in entry.imports:
                func_name = function.name.decode(errors="ignore") if function.name else "N/A"
                imported_functions.append(func_name)

                for category, apis in suspicious_api_calls.items():
                    if func_name in apis:
                        detected_suspicious_apis.add(func_name)

    return {
        "DLLs": imported_dlls,
        "Functions": imported_functions,
        "SuspiciousAPIs": list(detected_suspicious_apis)
    }

# 📌 Función para extraer strings sospechosas
def extract_strings(file_path):
    with open(file_path, "rb") as f:
        data = f.read()

    strings = re.findall(b"[ -~]{4,}", data)
    decoded_strings = [s.decode("utf-8", "ignore") for s in strings]

    suspicious_keywords = ["http", "ftp", "cmd", "powershell", "wget", "curl", "0.0.0.0"]
    suspicious_count = sum(1 for s in decoded_strings if any(k in s for k in suspicious_keywords))

    return suspicious_count

def detect_packing_by_size(sections):
    packed_sections = 0
    for section in sections:
        raw_size = section["RawSize"]
        virtual_size = section["VirtualSize"]

        # 📌 Un ejecutable empaquetado suele tener secciones con RawSize muy pequeño y VirtualSize grande
        if raw_size > 0 and virtual_size / raw_size > 10:  # Umbral ajustable
            packed_sections += 1

    return 1 if packed_sections > 0 else 0


# In[4]:


def extract_pe_info(file_path):
    try:
        pe = pefile.PE(file_path)
        imports_data = extract_imports(pe)

        # 📌 Evaluar secciones del ejecutable
        section_data = []
        for section in pe.sections:
            section_data.append({
                "Name": section.Name.rstrip(b'\x00').decode(errors="ignore"),
                "RawSize": section.SizeOfRawData,
                "VirtualSize": section.Misc_VirtualSize
            })
        

        pe_info = {
            "Filename": os.path.basename(file_path),
            "SHA256": get_sha256(file_path),
            "TimeDateStamp": convert_timestamp(pe.FILE_HEADER.TimeDateStamp),
            "Subsystem": pe.OPTIONAL_HEADER.Subsystem,
            "NumberOfSections": pe.FILE_HEADER.NumberOfSections,
            "Num_DLLs": len(imports_data["DLLs"]),
            "Num_Imports": len(imports_data["Functions"]),
            "Num_Suspicious_APIs": len(imports_data["SuspiciousAPIs"]),
            "Strings": extract_strings(file_path),
            "Packed_By_Size": detect_packing_by_size(section_data)  # 📌 Nueva evaluación
        }

        pe.close()
        return pe_info

    except Exception as e:
        print(f"⚠️ Error analizando {file_path}: {e}")
        return None


# In[5]:


dataset = []
for file in malware_files:
    if not os.path.isfile(file):
        continue
    pe_data = extract_pe_info(file)
    if pe_data:
        dataset.append(pe_data)

df = pd.DataFrame(dataset)

# 📌 Guardar dataset limpio
df.to_csv("malware_dataset_optimized.csv", index=False)
print("✅ Dataset optimizado guardado como 'malware_dataset_optimized.csv'")

# 📌 Mostrar las primeras filas
display(df.head())


# ## Exploración y pre-procesamiento de datos

# In[6]:


df = pd.read_csv("malware_dataset_optimized.csv")


# In[7]:


# Mostrar las primeras filas
display(df.head())

# Revisar los tipos de datos
print("\n📌 Tipos de datos en cada columna:")
print(df.dtypes)

# Revisar valores nulos
print("\n📌 Valores nulos en el dataset:")
print(df.isnull().sum())

# Revisar estadísticas de columnas numéricas
print("\n📌 Resumen estadístico de columnas numéricas:")
print(df.describe())

# Revisar estadísticas de columnas categóricas
print("\n📌 Resumen estadístico de columnas categóricas:")
print(df.describe(include="object"))


# In[8]:


from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
columns_to_scale = ["NumberOfSections", "Num_DLLs", "Num_Imports", "Num_Suspicious_APIs", "Strings"]

df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

print("✅ Columnas numéricas normalizadas correctamente")


# In[9]:


df = pd.get_dummies(df, columns=["Subsystem"], prefix="Subsystem")


# In[10]:


# Filtrar solo columnas numéricas antes de calcular la correlación
df_numeric = df.select_dtypes(include=['number'])

# Verificar qué columnas quedaron después del filtrado
print("📊 Columnas numéricas en el DataFrame:")
print(df_numeric.columns)


# In[11]:


import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.heatmap(df_numeric.corr(), annot=True, cmap="coolwarm")
plt.title("Mapa de correlación de variables")
plt.show()


# # Parte 2

# ## Implementación del modelo

# ### Algoritmo 1: K-Means

# In[12]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 📌 Cargar dataset
df = pd.read_csv("malware_dataset_optimized.csv")

# 📌 Escalar los datos
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.select_dtypes(include=['number']))

# 📌 Definir rango de valores de K para evaluar
K_range = range(1, 11)
inertia = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(df_scaled)
    inertia.append(kmeans.inertia_)

# 📌 Graficar el método del codo
plt.figure(figsize=(8, 5))
plt.plot(K_range, inertia, marker='o', linestyle='--', color='b')
plt.xlabel("Número de Clústeres (K)")
plt.ylabel("Inercia (Distorsión)")
plt.title("Método del Codo para K-Means")
plt.xticks(K_range)
plt.grid(True)
plt.show()


# El gráfico muestra la inercia (distorsión) en función del número de clústeres  K . Se observa una disminución rápida de la inercia hasta  K=4 , donde la curva comienza a aplanarse. Esto indica que 4 es un punto óptimo para la segmentación, ya que agregar más clústeres no reduce significativamente la inercia.

# ### Algortimo 2: Agglomerative Clustering

# In[13]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage
from sklearn.preprocessing import StandardScaler

# 📌 Cargar dataset
df = pd.read_csv("malware_dataset_optimized.csv")

# 📌 Escalar los datos
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.select_dtypes(include=['number']))

# 📌 Calcular la distancia intra-cluster con el método de Ward
Z = linkage(df_scaled, method="ward")

# 📌 Extraer los valores de la distancia intra-cluster
distances = Z[:, 2]  # La tercera columna en Z contiene las distancias intra-cluster

# 📌 Crear el gráfico del método del codo
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(distances) + 1), distances[::-1], marker='o', linestyle='--', color='b')  # Se invierte el orden para ver el "codo"
plt.xlabel("Número de Clústeres (K)")
plt.ylabel("Distancia intra-cluster")
plt.title("Método del Codo para Agglomerative Clustering")
plt.grid(True)
plt.show()


# En este gráfico, la distancia intra-cluster disminuye drásticamente hasta  K=8 , después de lo cual la curva se estabiliza. Esto sugiere que  K=8  podría ser una buena elección para este algoritmo, ya que permite capturar mejor la estructura de los datos sin sobre-segmentación.
# 

# In[15]:


from sklearn.cluster import AgglomerativeClustering

# 📌 Definir el número óptimo de clusters basado en la gráfica
optimal_k = 4  # Ajustar con la observación

# 📌 Aplicar Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=optimal_k, linkage="ward")
agglo_labels = agglo.fit_predict(df_scaled)

# 📌 Mostrar la cantidad de elementos en cada cluster
unique, counts = np.unique(agglo_labels, return_counts=True)
cluster_distribution = dict(zip(unique, counts))
print(f"📊 Distribución de malware por cluster: {cluster_distribution}")


# Basándonos en las gráficas obtenidas del método del codo para K-Means y Agglomerative Clustering, podemos notar que en K-Means la inercia disminuye rápidamente hasta K=4, donde la reducción se vuelve más leve, lo que indica que este es un número óptimo de clústeres según este método. En la visualización con K=4, se observa una clara separación entre los grupos, lo que confirma que esta cantidad es adecuada para nuestro conjunto de datos.
# 
# Por otro lado, el Agglomerative Clustering mostró una reducción significativa en la distancia intra-cluster hasta aproximadamente K=8, donde la curva comienza a estabilizarse. Esto sugiere que 8 podría ser un mejor número de clústeres para esta técnica, ya que permite capturar más variaciones dentro de los datos sin agrupar en exceso.
# 

# ## Coeficiente de Silhouette

# ### K-Means

# In[ ]:


from sklearn.metrics import silhouette_score

silhouette_scores = []

for k in range(2, 11):  # Silhouette no se puede calcular para K=1
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(df_scaled)
    score = silhouette_score(df_scaled, labels)
    silhouette_scores.append(score)

# 📌 Graficar el coeficiente de Silhouette
plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--', color='g')
plt.xlabel("Número de Clústeres (K)")
plt.ylabel("Coeficiente de Silhouette")
plt.title("Análisis del Coeficiente de Silhouette para K-Means")
plt.xticks(range(2, 11))
plt.grid(True)
plt.show()


# El coeficiente de Silhouette mide qué tan bien separadas están las agrupaciones. Su valor más alto se alcanza en  K=8 , lo que indica que esta cantidad de clústeres proporciona la mejor cohesión y separación dentro de los datos.
# 

# ### Agglomerative Clustering

# In[17]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

silhouette_scores = []

for k in range(2, 11):
    agglo = AgglomerativeClustering(n_clusters=k, linkage="ward")
    labels = agglo.fit_predict(df_scaled)
    score = silhouette_score(df_scaled, labels)
    silhouette_scores.append(score)

# 📌 Gráfica del Coeficiente de Silhouette
plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--', color='g')
plt.xlabel("Número de Clústeres (K)")
plt.ylabel("Coeficiente de Silhouette")
plt.title("Análisis del Coeficiente de Silhouette para Agglomerative Clustering")
plt.xticks(range(2, 11))
plt.grid(True)
plt.show()


# De manera similar, el coeficiente de Silhouette para Agglomerative Clustering alcanza su punto máximo en  K=8 . Sin embargo, a partir de  K=9 , el valor disminuye, lo que sugiere que más clústeres empiezan a sobreajustar los datos y generan agrupaciones menos definidas.

# ## Gemini

# In[26]:


import google.generativeai as genai
import pandas as pd
import numpy as np
from google.api_core import retry
from tqdm.auto import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
from dotenv import load_dotenv

# Cargar variables de entorno
dotenv_path = os.path.join(os.getcwd(), ".env.local") 
load_dotenv(dotenv_path, override=True)

print("env:",  os.getenv("GOOGLE_API_KEY"))

# Configurar Gemini API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

tqdm.pandas()  # Para ver el progreso en Pandas

# Función para generar embeddings con Gemini AI
def make_embed_text_fn(model):
    @retry.Retry(timeout=300.0)
    def embed_fn(text: str) -> list[float]:
        embedding = genai.embed_content(model=model, content=text, task_type="clustering")
        return embedding["embedding"]
    return embed_fn

# Cargar dataset
df = pd.read_csv("malware_dataset_optimized.csv")

# Convertir la columna de características en texto
df["Features"] = df["Num_Suspicious_APIs"].astype(str)

# Generar embeddings
model = "models/embedding-001"
df["Embeddings"] = df["Features"].progress_apply(make_embed_text_fn(model))

# Convertir a matriz NumPy
X = np.array(df["Embeddings"].to_list(), dtype=np.float32)

# Normalizar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("✅ Embeddings generados y normalizados correctamente.")


# In[ ]:


# Aplicar K-Means con K=8 (según el coeficiente de Silhouette)
optimal_k = 8
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df["Family"] = kmeans.fit_predict(X_scaled)

# Convertir las etiquetas en nombres de familia
df["Family"] = df["Family"].apply(lambda x: f"Fam{x}")

# Mostrar cuántos elementos hay en cada familia
print("📊 Distribución de malware por familia:")
print(df["Family"].value_counts())


# In[29]:


# Aplicar reducción de dimensionalidad con t-SNE
tsne = TSNE(random_state=42, perplexity=5.0, n_iter=1000)
tsne_results = tsne.fit_transform(X_scaled)

# Crear DataFrame con los resultados de t-SNE
df_tsne = pd.DataFrame(tsne_results, columns=["TSNE1", "TSNE2"])
df_tsne["Family"] = df["Family"]

# Visualización de clusters con t-SNE
plt.figure(figsize=(8, 6))
sns.set_style("darkgrid", {"grid.color": ".6", "grid.linestyle": ":"})
sns.scatterplot(data=df_tsne, x="TSNE1", y="TSNE2", hue="Family", palette="hls", edgecolor="k", alpha=0.7)
plt.title("Clusters de Malware usando Embeddings de Gemini + t-SNE (Agrupados por Familia)")
plt.xlabel("TSNE1")
plt.ylabel("TSNE2")
plt.legend(title="Familia", bbox_to_anchor=(1, 1))
plt.show()

# Guardar el dataset con embeddings y reducción de dimensionalidad
df_tsne.to_csv("malware_families_tsne.csv", index=False)
print("✅ Dataset con embeddings y t-SNE guardado como 'malware_families_tsne.csv'")


# In[28]:


# Crear tabla con valores de t-SNE y la familia asignada
df_tsne_display = df_tsne.sample(20)  # Mostrar solo 20 filas aleatorias

# Imprimir tabla en formato bonito
import IPython.display as display
display.display(df_tsne_display)


# # Análisis de similitud

# ### Grafos de todo el conjunto

# In[ ]:


import pandas as pd
import networkx as nx
from sklearn.metrics import jaccard_score
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity

# Cargar el archivo CSV
file_path = 'malware_dataset_optimized.csv'
df = pd.read_csv(file_path)

# Mostrar las primeras filas para entender la estructura de los datos
print(df.head())

# Preparar los datos: Convertimos las columnas 'Strings' y 'Num_Imports' a sets para calcular el índice de Jaccard
df['Strings'] = df['Strings'].apply(lambda x: set(str(x).split(',')))  # Si hay un separador específico en 'Strings'
df['Num_Imports'] = df['Num_Imports'].apply(lambda x: set([x]))  # Convertimos el número de importaciones a un conjunto


# In[ ]:


# Función para calcular el índice de Jaccard entre dos conjuntos
def jaccard_index(set1, set2):
    return len(set1.intersection(set2)) / len(set1.union(set2))

# Función para construir un grafo de similitudes para una familia
def build_graph(df_family, threshold=0.5, feature='Strings'):
    G = nx.Graph()
    
    # Comparar cada par de malwares
    for i, row1 in df_family.iterrows():
        for j, row2 in df_family.iterrows():
            if i >= j:
                continue
            sim = jaccard_index(row1[feature], row2[feature])
            if sim >= threshold:
                G.add_edge(row1['SHA256'], row2['SHA256'], weight=sim)

    return G

# Función para construir un grafo para todo el conjunto de malwares
def build_global_graph(df, threshold=0.5, feature='Strings'):
    G = nx.Graph()

    # Comparar cada par de malwares
    for i, row1 in df.iterrows():
        for j, row2 in df.iterrows():
            if i >= j:
                continue
            sim = jaccard_index(row1[feature], row2[feature])
            if sim >= threshold:
                G.add_edge(row1['SHA256'], row2['SHA256'], weight=sim)

    return G


# In[ ]:


def draw_graph_with_layout(graph, title):
    # Usar un layout para distribuir los nodos
    pos = nx.spring_layout(graph, k=0.1, iterations=50)  # `k` controla la distancia entre los nodos
    
    # Dibujar el grafo
    plt.figure(figsize=(12, 8))
    nx.draw(graph, pos, with_labels=True, node_size=200, font_size=8, node_color='lightblue', edge_color='gray')
    plt.title(title)
    plt.show()

# Crear un grafo global
global_graph = build_global_graph(df, threshold=0.5, feature='Strings')

# Visualizar el grafo global original
draw_graph_with_layout(global_graph, "Grafo de similitud global")

# Obtener los componentes conectados
components = list(nx.connected_components(global_graph))

# Dibujar un grafo para cada componente conectado
for idx, component in enumerate(components):
    subgraph = global_graph.subgraph(component)
    draw_graph_with_layout(subgraph, f"Grafo de similitud global - Componente {idx + 1}")


# ### Grafos por familia

# In[ ]:


# Cargar el archivo de malwares y el archivo de familias
file_path_malware = 'malware_dataset_optimized.csv'
file_path_families = 'malware_dataset_families.csv'

# Cargar los datos de los malwares y de las familias
df_malware = pd.read_csv(file_path_malware)
df_families = pd.read_csv(file_path_families)

# Realizar el merge entre los archivos utilizando 'Filename'
df_merged = pd.merge(df_malware, df_families[['Filename', 'Family', 'Embeddings']], on='Filename', how='left')


# In[ ]:


# Función para calcular la similitud coseno entre dos vectores de embeddings
def calculate_cosine_similarity(embedding1, embedding2):
    return cosine_similarity([embedding1], [embedding2])[0][0]

# Función para construir un grafo de similitudes por familia usando los embeddings
def build_graph_by_family(df_family, threshold=0.8):
    G = nx.Graph()
    
    # Comparar cada par de malwares en la familia
    for i, row1 in df_family.iterrows():
        for j, row2 in df_family.iterrows():
            if i >= j:
                continue
            # Calcular la similitud coseno entre los embeddings
            embedding1 = np.array(eval(row1['Embeddings']))  # Convertir el string de la lista en un array
            embedding2 = np.array(eval(row2['Embeddings']))
            sim = calculate_cosine_similarity(embedding1, embedding2)
            
            # Si la similitud es mayor o igual que el umbral, agregar una arista entre los nodos
            if sim >= threshold:
                G.add_edge(row1['SHA256'], row2['SHA256'], weight=sim)

    return G


# In[ ]:


# Crear un grafo por familia
graphs_by_family = {}
for family in df_merged['Family'].unique():
    df_family = df_merged[df_merged['Family'] == family]
    graphs_by_family[family] = build_graph_by_family(df_family, threshold=0.8)

# Función para visualizar los grafos con un layout adecuado
def draw_graph_with_layout(graph, title):
    pos = nx.spring_layout(graph, k=0.1, iterations=50)
    plt.figure(figsize=(12, 8))
    nx.draw(graph, pos, with_labels=True, node_size=200, font_size=8, node_color='lightblue', edge_color='gray')
    plt.title(title)
    plt.show()

# Visualizar los grafos por familia
for family, graph in graphs_by_family.items():
    draw_graph_with_layout(graph, f"Grafo de similitud para la familia {family}")


# Conclusiones
# 
# 1. Para ambos algoritmos, ¿para qué número de clústeres se obtiene el coeficiente de Silhouette más alto?
# 
# Silhouette alcanza su valor más alto en 8 clusters. Tiene un valor cercano a 0.95. 
# 
# 2. Para ambos algoritmos, ¿En que medida coincide el coeficiente de Silhouette con el método del codo?
# 
# 
# 
# 3. Según los resultados obtenidos de ambos algoritmos ¿Cuántas familias cree que existen entre los ejemplares de malware proporcionados?
# 
# Al analizar los resultados, existen aproximadamente 5 familias en el malware proporcionado.  
# 
# 4. ¿En qué medida coincide el análisis de similitud con las familias encontradas utilizando los algoritmos de partición, para ambas características (strings, llamadas a las funciones)?
# 
# El análisis de similitud coincide con las familias encontradas usando el alforitmo de partición (t-sne). Ambos métodos nos dicen que hay 5 familias de malware. 
