#!/usr/bin/env python
# coding: utf-8

# # Laboratorio #4 - Familias de Malware

# # Parte 1

# In[1]:


import os
import pefile
import pandas as pd
import hashlib
import re
from datetime import datetime, timezone
import subprocess


# ## Creación del dataset

# In[2]:


MALWARE_DIR = "./MALWR"
malware_files = [os.path.join(MALWARE_DIR, f) for f in os.listdir(MALWARE_DIR) if os.path.isfile(os.path.join(MALWARE_DIR, f))]

print(f"🔍 Se encontraron {len(malware_files)} archivos de malware en el directorio.")


# In[3]:


# 📌 Función para calcular SHA-256
def get_sha256(file_path):
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

# 📌 Función para extraer timestamps
def convert_timestamp(timestamp):
    try:
        return datetime.fromtimestamp(timestamp, timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
    except (OSError, OverflowError, ValueError):
        return "Invalid Timestamp"

# 📌 API sospechosas
suspicious_api_calls = {
    "process hollowing": {
        "CreateProcess", "GetModuleHandle", "GetProcAddress", "VirtualAllocEx", "WriteProcessMemory", "SetThreadContext", "ResumeThread"
    },
    "create remote thread": {
        "OpenProcess", "GetModuleHandle", "GetProcAddress", "VirtualAllocEx", "WriteProcessMemory", "CreateRemoteThread"
    },
    "enumerating processes": {
        "CreateToolhelp32Snapshot", "Process32First", "Process32Next", "WTSEnumerateProcesses"
    },
    "drop file from PE resource": {
        "GetModuleHandle", "FindResource", "LoadResource", "CreateFileA"
    },
    "IAT hooking": {
        "GetModuleHandle", "strcmp", "VirtualProtect"
    },
    "delete itself": {
        "GetModuleFileName", "ExitProcess", "DeleteFile"
    },
    "download and execute PE file": {
        "URLDownloadToFile", "ShellExecute"
    },
    "bind TCP port": {
        "WSAStartup", "socket"
    },
    "capture network traffic": {
        "socket", "bind", "WSAIoctl", "recvfrom"
    }
}

# 📌 Función para extraer importaciones
def extract_imports(pe):
    imported_functions = []
    imported_dlls = []
    detected_suspicious_apis = set()

    if hasattr(pe, "DIRECTORY_ENTRY_IMPORT"):
        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            dll_name = entry.dll.decode(errors="ignore")
            imported_dlls.append(dll_name)

            for function in entry.imports:
                func_name = function.name.decode(errors="ignore") if function.name else "N/A"
                imported_functions.append(func_name)

                for category, apis in suspicious_api_calls.items():
                    if func_name in apis:
                        detected_suspicious_apis.add(func_name)

    return {
        "DLLs": imported_dlls,
        "Functions": imported_functions,
        "SuspiciousAPIs": list(detected_suspicious_apis)
    }

# 📌 Función para extraer strings sospechosas
def extract_strings(file_path):
    with open(file_path, "rb") as f:
        data = f.read()

    strings = re.findall(b"[ -~]{4,}", data)
    decoded_strings = [s.decode("utf-8", "ignore") for s in strings]

    suspicious_keywords = ["http", "ftp", "cmd", "powershell", "wget", "curl", "0.0.0.0"]
    suspicious_count = sum(1 for s in decoded_strings if any(k in s for k in suspicious_keywords))

    return suspicious_count

def detect_packing_by_size(sections):
    packed_sections = 0
    for section in sections:
        raw_size = section["RawSize"]
        virtual_size = section["VirtualSize"]

        # 📌 Un ejecutable empaquetado suele tener secciones con RawSize muy pequeño y VirtualSize grande
        if raw_size > 0 and virtual_size / raw_size > 10:  # Umbral ajustable
            packed_sections += 1

    return 1 if packed_sections > 0 else 0


# In[4]:


def extract_pe_info(file_path):
    try:
        pe = pefile.PE(file_path)
        imports_data = extract_imports(pe)

        # 📌 Evaluar secciones del ejecutable
        section_data = []
        for section in pe.sections:
            section_data.append({
                "Name": section.Name.rstrip(b'\x00').decode(errors="ignore"),
                "RawSize": section.SizeOfRawData,
                "VirtualSize": section.Misc_VirtualSize
            })
        

        pe_info = {
            "Filename": os.path.basename(file_path),
            "SHA256": get_sha256(file_path),
            "TimeDateStamp": convert_timestamp(pe.FILE_HEADER.TimeDateStamp),
            "Subsystem": pe.OPTIONAL_HEADER.Subsystem,
            "NumberOfSections": pe.FILE_HEADER.NumberOfSections,
            "Num_DLLs": len(imports_data["DLLs"]),
            "Num_Imports": len(imports_data["Functions"]),
            "Num_Suspicious_APIs": len(imports_data["SuspiciousAPIs"]),
            "Strings": extract_strings(file_path),
            "Packed_By_Size": detect_packing_by_size(section_data)  # 📌 Nueva evaluación
        }

        pe.close()
        return pe_info

    except Exception as e:
        print(f"⚠️ Error analizando {file_path}: {e}")
        return None


# In[5]:


dataset = []
for file in malware_files:
    if not os.path.isfile(file):
        continue
    pe_data = extract_pe_info(file)
    if pe_data:
        dataset.append(pe_data)

df = pd.DataFrame(dataset)

# 📌 Guardar dataset limpio
df.to_csv("malware_dataset_optimized.csv", index=False)
print("✅ Dataset optimizado guardado como 'malware_dataset_optimized.csv'")

# 📌 Mostrar las primeras filas
display(df.head())


# ## Exploración y pre-procesamiento de datos

# In[6]:


df = pd.read_csv("malware_dataset_optimized.csv")


# In[7]:


# Mostrar las primeras filas
display(df.head())

# Revisar los tipos de datos
print("\n📌 Tipos de datos en cada columna:")
print(df.dtypes)

# Revisar valores nulos
print("\n📌 Valores nulos en el dataset:")
print(df.isnull().sum())

# Revisar estadísticas de columnas numéricas
print("\n📌 Resumen estadístico de columnas numéricas:")
print(df.describe())

# Revisar estadísticas de columnas categóricas
print("\n📌 Resumen estadístico de columnas categóricas:")
print(df.describe(include="object"))


# In[8]:


from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
columns_to_scale = ["NumberOfSections", "Num_DLLs", "Num_Imports", "Num_Suspicious_APIs", "Strings"]

df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

print("✅ Columnas numéricas normalizadas correctamente")


# In[9]:


df = pd.get_dummies(df, columns=["Subsystem"], prefix="Subsystem")


# In[10]:


# Filtrar solo columnas numéricas antes de calcular la correlación
df_numeric = df.select_dtypes(include=['number'])

# Verificar qué columnas quedaron después del filtrado
print("📊 Columnas numéricas en el DataFrame:")
print(df_numeric.columns)


# In[11]:


import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.heatmap(df_numeric.corr(), annot=True, cmap="coolwarm")
plt.title("Mapa de correlación de variables")
plt.show()


# # Parte 2

# ## Implementación del modelo

# ### Algoritmo 1

# In[19]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn_extra.cluster import KMedoids
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA

# 📌 Cargar dataset
df = pd.read_csv("malware_dataset_optimized.csv")

# 📌 Filtrar solo columnas numéricas para clustering
df_numeric = df.select_dtypes(include=['number'])

# 📌 Escalar los datos para mejorar el rendimiento de los algoritmos de clustering
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_numeric)

# 📌 Definir rango de valores de K para evaluar
K_range = range(1, 11)

# 📌 Almacenar las distorsiones (inercia) para K-Means
inertia = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(df_scaled)
    inertia.append(kmeans.inertia_)

# 📌 Gráfica del Método del Codo para K-Means
plt.figure(figsize=(8,5))
plt.plot(K_range, inertia, marker='o', linestyle='--', color='b')
plt.xlabel("Número de Clústeres (K)")
plt.ylabel("Inercia (Distorsión)")
plt.title("Método del Codo para K-Means")
plt.xticks(K_range)
plt.grid(True)
plt.show()

# 📌 Aplicar reducción de dimensionalidad con PCA
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_scaled)

# 📌 Aplicar K-Means con el número óptimo de clusters (supongamos K=4)
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(df_scaled)

# 📌 Visualización de Clusters de K-Means con PCA
plt.figure(figsize=(8, 5))
plt.scatter(df_pca[:, 0], df_pca[:, 1], c=kmeans_labels, cmap="rainbow", edgecolors="k", alpha=0.7)
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.title(f"Clusters detectados con K-Means (K={optimal_k})")
plt.colorbar(label="Cluster ID")
plt.show()

# 📌 Ajustar DBSCAN con valores más óptimos
dbscan = DBSCAN(eps=0.5, min_samples=3)  # Se puede ajustar eps y min_samples
dbscan_labels = dbscan.fit_predict(df_scaled)

# 📌 Contar número de clusters encontrados (ignorando ruido -1)
num_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print(f"🔍 Número de clusters encontrados con DBSCAN: {num_clusters}")

# 📌 Visualización de Clusters de DBSCAN con PCA
plt.figure(figsize=(8, 5))
plt.scatter(df_pca[:, 0], df_pca[:, 1], c=dbscan_labels, cmap="rainbow", edgecolors="k", alpha=0.7)
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.title("Clusters detectados con DBSCAN (PCA)")
plt.colorbar(label="Cluster ID")
plt.show()


# Basándonos en las gráficas obtenidas del método del codo para K-Means, podemos notar que la inercia disminuye rápidamente hasta K=4 y luego la reducción es mucho más leve. Este es el punto donde se encuentra el “codo”, lo que indica que 4 es el número óptimo de clústeres según este método. En la visualización de K-Means con K=4, se observa una separación clara entre los grupos, lo que confirma que esta cantidad es adecuada para nuestra data.
# 
# Por otro lado, DBSCAN encontró 5 clústeres, lo que sugiere que algunos datos tienen una densidad diferente y podrían representar variaciones dentro de las familias de malware. Sin embargo, DBSCAN también detectó ruido (-1), lo que significa que algunos puntos no se agrupan bien en ningún clúster.
# 
# Aunque DBSCAN identifica más detalles en la estructura de los datos, la técnica de K-Means es más consistente en su agrupación. Por eso, elegimos K=4 como el número óptimo de clústeres, ya que es el punto donde logramos la mejor segmentación sin agrupar datos de forma forzada o generar demasiado ruido.

# ## Coeficiente de Silhouette

# In[20]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

# 📌 Cargar dataset
df = pd.read_csv("malware_dataset_optimized.csv")

# 📌 Filtrar solo columnas numéricas para clustering
df_numeric = df.select_dtypes(include=['number'])

# 📌 Escalar los datos para mejorar el rendimiento de los algoritmos de clustering
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_numeric)

# 📌 Definir rango de valores de K para evaluar
K_range = range(2, 11)  # El coeficiente de Silhouette no se puede calcular para K=1

# 📌 Almacenar los valores del coeficiente de Silhouette
silhouette_scores = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(df_scaled)
    score = silhouette_score(df_scaled, labels)
    silhouette_scores.append(score)

# 📌 Gráfica del Coeficiente de Silhouette
plt.figure(figsize=(8, 5))
plt.plot(K_range, silhouette_scores, marker='o', linestyle='--', color='g')
plt.xlabel("Número de Clústeres (K)")
plt.ylabel("Coeficiente de Silhouette")
plt.title("Análisis del Coeficiente de Silhouette para K-Means")
plt.xticks(K_range)
plt.grid(True)
plt.show()

# 📌 Determinar el mejor número de clústeres según el coeficiente de Silhouette
optimal_k_silhouette = K_range[np.argmax(silhouette_scores)]
print(f"🔍 Mejor número de clústeres según Silhouette: {optimal_k_silhouette}")


# ## Gemini

# In[25]:


import google.generativeai as genai
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from google.api_core import retry
from tqdm.auto import tqdm
from dotenv import load_dotenv

# 📌 Cargar variables de entorno para la API Key de Gemini
env_path = os.path.join(os.getcwd(), '.env.local')
load_dotenv(dotenv_path=env_path, override=True)

# 📌 Configurar Gemini API
genai.configure(api_key=os.getenv("GENAI_API_KEY"))

tqdm.pandas()  # Para visualización de progreso en Pandas

# 📌 Función para generar embeddings con Gemini AI
def make_embed_text_fn(model):
    @retry.Retry(timeout=300.0)
    def embed_fn(text: str) -> list[float]:
        embedding = genai.embed_content(model=model, content=text, task_type="clustering")
        return embedding["embedding"]
    return embed_fn

# 📌 Cargar dataset de familias de malware
df = pd.read_csv("greencat_families.csv")

# 📌 Verificar las columnas disponibles
print("📊 Columnas del dataset:", df.columns)

# 📌 Convertir en texto los valores relevantes para embeddings
df["Functions"] = df["Functions"].astype(str)  

# 📌 Generar embeddings usando Gemini AI
model = 'models/embedding-001'
df["Embeddings"] = df["Functions"].progress_apply(make_embed_text_fn(model))

# 📌 Convertir embeddings a numpy array
X = np.array(df["Embeddings"].to_list(), dtype=np.float32)

# 📌 Normalizar los embeddings antes del clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 📌 Aplicar reducción de dimensionalidad con t-SNE para visualizar en 2D
tsne = TSNE(random_state=42, perplexity=5.0, n_iter=1000)
tsne_results = tsne.fit_transform(X_scaled)

# 📌 Crear DataFrame con los resultados de t-SNE
df_tsne = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'])
df_tsne["Family"] = df["Family"]  # Asignar etiquetas de familia

# 📌 Visualización de las familias de malware con t-SNE
plt.figure(figsize=(8, 6))
sns.set_style("darkgrid", {"grid.color": ".6", "grid.linestyle": ":"})
sns.scatterplot(data=df_tsne, x="TSNE1", y="TSNE2", hue="Family", palette="hls", edgecolor="k", alpha=0.7)
plt.title("Clusters de Malware usando Embeddings de Gemini + t-SNE (Agrupados por Familia)")
plt.xlabel("TSNE1")
plt.ylabel("TSNE2")
plt.legend(title="Familia", bbox_to_anchor=(1, 1))
plt.show()

# 📌 Guardar dataset con embeddings y reducción de dimensionalidad
df_tsne.to_csv("malware_families_tsne.csv", index=False)
print("✅ Dataset con embeddings y t-SNE guardado como 'malware_families_tsne.csv'")

