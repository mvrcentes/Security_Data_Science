#!/usr/bin/env python
# coding: utf-8

# # Laboratorio #4 - Familias de Malware

# # Parte 1

# In[1]:


import os
import pefile
import pandas as pd
import hashlib
import re
from datetime import datetime, timezone
import subprocess


# ## Creaci√≥n del dataset

# In[2]:


MALWARE_DIR = "./MALWR"
malware_files = [os.path.join(MALWARE_DIR, f) for f in os.listdir(MALWARE_DIR) if os.path.isfile(os.path.join(MALWARE_DIR, f))]

print(f"üîç Se encontraron {len(malware_files)} archivos de malware en el directorio.")


# In[3]:


# üìå Funci√≥n para calcular SHA-256
def get_sha256(file_path):
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

# üìå Funci√≥n para extraer timestamps
def convert_timestamp(timestamp):
    try:
        return datetime.fromtimestamp(timestamp, timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
    except (OSError, OverflowError, ValueError):
        return "Invalid Timestamp"

# üìå API sospechosas
suspicious_api_calls = {
    "process hollowing": {
        "CreateProcess", "GetModuleHandle", "GetProcAddress", "VirtualAllocEx", "WriteProcessMemory", "SetThreadContext", "ResumeThread"
    },
    "create remote thread": {
        "OpenProcess", "GetModuleHandle", "GetProcAddress", "VirtualAllocEx", "WriteProcessMemory", "CreateRemoteThread"
    },
    "enumerating processes": {
        "CreateToolhelp32Snapshot", "Process32First", "Process32Next", "WTSEnumerateProcesses"
    },
    "drop file from PE resource": {
        "GetModuleHandle", "FindResource", "LoadResource", "CreateFileA"
    },
    "IAT hooking": {
        "GetModuleHandle", "strcmp", "VirtualProtect"
    },
    "delete itself": {
        "GetModuleFileName", "ExitProcess", "DeleteFile"
    },
    "download and execute PE file": {
        "URLDownloadToFile", "ShellExecute"
    },
    "bind TCP port": {
        "WSAStartup", "socket"
    },
    "capture network traffic": {
        "socket", "bind", "WSAIoctl", "recvfrom"
    }
}

# üìå Funci√≥n para extraer importaciones
def extract_imports(pe):
    imported_functions = []
    imported_dlls = []
    detected_suspicious_apis = set()

    if hasattr(pe, "DIRECTORY_ENTRY_IMPORT"):
        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            dll_name = entry.dll.decode(errors="ignore")
            imported_dlls.append(dll_name)

            for function in entry.imports:
                func_name = function.name.decode(errors="ignore") if function.name else "N/A"
                imported_functions.append(func_name)

                for category, apis in suspicious_api_calls.items():
                    if func_name in apis:
                        detected_suspicious_apis.add(func_name)

    return {
        "DLLs": imported_dlls,
        "Functions": imported_functions,
        "SuspiciousAPIs": list(detected_suspicious_apis)
    }

# üìå Funci√≥n para extraer strings sospechosas
def extract_strings(file_path):
    with open(file_path, "rb") as f:
        data = f.read()

    strings = re.findall(b"[ -~]{4,}", data)
    decoded_strings = [s.decode("utf-8", "ignore") for s in strings]

    suspicious_keywords = ["http", "ftp", "cmd", "powershell", "wget", "curl", "0.0.0.0"]
    suspicious_count = sum(1 for s in decoded_strings if any(k in s for k in suspicious_keywords))

    return suspicious_count

def detect_packing_by_size(sections):
    packed_sections = 0
    for section in sections:
        raw_size = section["RawSize"]
        virtual_size = section["VirtualSize"]

        # üìå Un ejecutable empaquetado suele tener secciones con RawSize muy peque√±o y VirtualSize grande
        if raw_size > 0 and virtual_size / raw_size > 10:  # Umbral ajustable
            packed_sections += 1

    return 1 if packed_sections > 0 else 0


# In[4]:


def extract_pe_info(file_path):
    try:
        pe = pefile.PE(file_path)
        imports_data = extract_imports(pe)

        # üìå Evaluar secciones del ejecutable
        section_data = []
        for section in pe.sections:
            section_data.append({
                "Name": section.Name.rstrip(b'\x00').decode(errors="ignore"),
                "RawSize": section.SizeOfRawData,
                "VirtualSize": section.Misc_VirtualSize
            })
        

        pe_info = {
            "Filename": os.path.basename(file_path),
            "SHA256": get_sha256(file_path),
            "TimeDateStamp": convert_timestamp(pe.FILE_HEADER.TimeDateStamp),
            "Subsystem": pe.OPTIONAL_HEADER.Subsystem,
            "NumberOfSections": pe.FILE_HEADER.NumberOfSections,
            "Num_DLLs": len(imports_data["DLLs"]),
            "Num_Imports": len(imports_data["Functions"]),
            "Num_Suspicious_APIs": len(imports_data["SuspiciousAPIs"]),
            "Strings": extract_strings(file_path),
            "Packed_By_Size": detect_packing_by_size(section_data)  # üìå Nueva evaluaci√≥n
        }

        pe.close()
        return pe_info

    except Exception as e:
        print(f"‚ö†Ô∏è Error analizando {file_path}: {e}")
        return None


# In[5]:


dataset = []
for file in malware_files:
    if not os.path.isfile(file):
        continue
    pe_data = extract_pe_info(file)
    if pe_data:
        dataset.append(pe_data)

df = pd.DataFrame(dataset)

# üìå Guardar dataset limpio
df.to_csv("malware_dataset_optimized.csv", index=False)
print("‚úÖ Dataset optimizado guardado como 'malware_dataset_optimized.csv'")

# üìå Mostrar las primeras filas
display(df.head())


# ## Exploraci√≥n y pre-procesamiento de datos

# In[6]:


df = pd.read_csv("malware_dataset_optimized.csv")


# In[7]:


# Mostrar las primeras filas
display(df.head())

# Revisar los tipos de datos
print("\nüìå Tipos de datos en cada columna:")
print(df.dtypes)

# Revisar valores nulos
print("\nüìå Valores nulos en el dataset:")
print(df.isnull().sum())

# Revisar estad√≠sticas de columnas num√©ricas
print("\nüìå Resumen estad√≠stico de columnas num√©ricas:")
print(df.describe())

# Revisar estad√≠sticas de columnas categ√≥ricas
print("\nüìå Resumen estad√≠stico de columnas categ√≥ricas:")
print(df.describe(include="object"))


# In[8]:


from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
columns_to_scale = ["NumberOfSections", "Num_DLLs", "Num_Imports", "Num_Suspicious_APIs", "Strings"]

df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

print("‚úÖ Columnas num√©ricas normalizadas correctamente")


# In[9]:


df = pd.get_dummies(df, columns=["Subsystem"], prefix="Subsystem")


# In[10]:


# Filtrar solo columnas num√©ricas antes de calcular la correlaci√≥n
df_numeric = df.select_dtypes(include=['number'])

# Verificar qu√© columnas quedaron despu√©s del filtrado
print("üìä Columnas num√©ricas en el DataFrame:")
print(df_numeric.columns)


# In[11]:


import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.heatmap(df_numeric.corr(), annot=True, cmap="coolwarm")
plt.title("Mapa de correlaci√≥n de variables")
plt.show()


# # Parte 2

# ## Implementaci√≥n del modelo

# ### Algoritmo 1: K-Means

# In[12]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# üìå Cargar dataset
df = pd.read_csv("malware_dataset_optimized.csv")

# üìå Escalar los datos
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.select_dtypes(include=['number']))

# üìå Definir rango de valores de K para evaluar
K_range = range(1, 11)
inertia = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(df_scaled)
    inertia.append(kmeans.inertia_)

# üìå Graficar el m√©todo del codo
plt.figure(figsize=(8, 5))
plt.plot(K_range, inertia, marker='o', linestyle='--', color='b')
plt.xlabel("N√∫mero de Cl√∫steres (K)")
plt.ylabel("Inercia (Distorsi√≥n)")
plt.title("M√©todo del Codo para K-Means")
plt.xticks(K_range)
plt.grid(True)
plt.show()


# El gr√°fico muestra la inercia (distorsi√≥n) en funci√≥n del n√∫mero de cl√∫steres  K . Se observa una disminuci√≥n r√°pida de la inercia hasta  K=4 , donde la curva comienza a aplanarse. Esto indica que 4 es un punto √≥ptimo para la segmentaci√≥n, ya que agregar m√°s cl√∫steres no reduce significativamente la inercia.

# ### Algortimo 2: Agglomerative Clustering

# In[13]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage
from sklearn.preprocessing import StandardScaler

# üìå Cargar dataset
df = pd.read_csv("malware_dataset_optimized.csv")

# üìå Escalar los datos
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.select_dtypes(include=['number']))

# üìå Calcular la distancia intra-cluster con el m√©todo de Ward
Z = linkage(df_scaled, method="ward")

# üìå Extraer los valores de la distancia intra-cluster
distances = Z[:, 2]  # La tercera columna en Z contiene las distancias intra-cluster

# üìå Crear el gr√°fico del m√©todo del codo
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(distances) + 1), distances[::-1], marker='o', linestyle='--', color='b')  # Se invierte el orden para ver el "codo"
plt.xlabel("N√∫mero de Cl√∫steres (K)")
plt.ylabel("Distancia intra-cluster")
plt.title("M√©todo del Codo para Agglomerative Clustering")
plt.grid(True)
plt.show()


# En este gr√°fico, la distancia intra-cluster disminuye dr√°sticamente hasta  K=8 , despu√©s de lo cual la curva se estabiliza. Esto sugiere que  K=8  podr√≠a ser una buena elecci√≥n para este algoritmo, ya que permite capturar mejor la estructura de los datos sin sobre-segmentaci√≥n.
# 

# In[15]:


from sklearn.cluster import AgglomerativeClustering

# üìå Definir el n√∫mero √≥ptimo de clusters basado en la gr√°fica
optimal_k = 4  # Ajustar con la observaci√≥n

# üìå Aplicar Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=optimal_k, linkage="ward")
agglo_labels = agglo.fit_predict(df_scaled)

# üìå Mostrar la cantidad de elementos en cada cluster
unique, counts = np.unique(agglo_labels, return_counts=True)
cluster_distribution = dict(zip(unique, counts))
print(f"üìä Distribuci√≥n de malware por cluster: {cluster_distribution}")


# Bas√°ndonos en las gr√°ficas obtenidas del m√©todo del codo para K-Means y Agglomerative Clustering, podemos notar que en K-Means la inercia disminuye r√°pidamente hasta K=4, donde la reducci√≥n se vuelve m√°s leve, lo que indica que este es un n√∫mero √≥ptimo de cl√∫steres seg√∫n este m√©todo. En la visualizaci√≥n con K=4, se observa una clara separaci√≥n entre los grupos, lo que confirma que esta cantidad es adecuada para nuestro conjunto de datos.
# 
# Por otro lado, el Agglomerative Clustering mostr√≥ una reducci√≥n significativa en la distancia intra-cluster hasta aproximadamente K=8, donde la curva comienza a estabilizarse. Esto sugiere que 8 podr√≠a ser un mejor n√∫mero de cl√∫steres para esta t√©cnica, ya que permite capturar m√°s variaciones dentro de los datos sin agrupar en exceso.
# 

# ## Coeficiente de Silhouette

# ### K-Means

# In[ ]:


from sklearn.metrics import silhouette_score

silhouette_scores = []

for k in range(2, 11):  # Silhouette no se puede calcular para K=1
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(df_scaled)
    score = silhouette_score(df_scaled, labels)
    silhouette_scores.append(score)

# üìå Graficar el coeficiente de Silhouette
plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--', color='g')
plt.xlabel("N√∫mero de Cl√∫steres (K)")
plt.ylabel("Coeficiente de Silhouette")
plt.title("An√°lisis del Coeficiente de Silhouette para K-Means")
plt.xticks(range(2, 11))
plt.grid(True)
plt.show()


# El coeficiente de Silhouette mide qu√© tan bien separadas est√°n las agrupaciones. Su valor m√°s alto se alcanza en  K=8 , lo que indica que esta cantidad de cl√∫steres proporciona la mejor cohesi√≥n y separaci√≥n dentro de los datos.
# 

# ### Agglomerative Clustering

# In[17]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

silhouette_scores = []

for k in range(2, 11):
    agglo = AgglomerativeClustering(n_clusters=k, linkage="ward")
    labels = agglo.fit_predict(df_scaled)
    score = silhouette_score(df_scaled, labels)
    silhouette_scores.append(score)

# üìå Gr√°fica del Coeficiente de Silhouette
plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--', color='g')
plt.xlabel("N√∫mero de Cl√∫steres (K)")
plt.ylabel("Coeficiente de Silhouette")
plt.title("An√°lisis del Coeficiente de Silhouette para Agglomerative Clustering")
plt.xticks(range(2, 11))
plt.grid(True)
plt.show()


# De manera similar, el coeficiente de Silhouette para Agglomerative Clustering alcanza su punto m√°ximo en  K=8 . Sin embargo, a partir de  K=9 , el valor disminuye, lo que sugiere que m√°s cl√∫steres empiezan a sobreajustar los datos y generan agrupaciones menos definidas.

# ## Gemini

# In[26]:


import google.generativeai as genai
import pandas as pd
import numpy as np
from google.api_core import retry
from tqdm.auto import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
from dotenv import load_dotenv

# Cargar variables de entorno
dotenv_path = os.path.join(os.getcwd(), ".env.local") 
load_dotenv(dotenv_path, override=True)

print("env:",  os.getenv("GOOGLE_API_KEY"))

# Configurar Gemini API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

tqdm.pandas()  # Para ver el progreso en Pandas

# Funci√≥n para generar embeddings con Gemini AI
def make_embed_text_fn(model):
    @retry.Retry(timeout=300.0)
    def embed_fn(text: str) -> list[float]:
        embedding = genai.embed_content(model=model, content=text, task_type="clustering")
        return embedding["embedding"]
    return embed_fn

# Cargar dataset
df = pd.read_csv("malware_dataset_optimized.csv")

# Convertir la columna de caracter√≠sticas en texto
df["Features"] = df["Num_Suspicious_APIs"].astype(str)

# Generar embeddings
model = "models/embedding-001"
df["Embeddings"] = df["Features"].progress_apply(make_embed_text_fn(model))

# Convertir a matriz NumPy
X = np.array(df["Embeddings"].to_list(), dtype=np.float32)

# Normalizar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("‚úÖ Embeddings generados y normalizados correctamente.")


# In[ ]:


# Aplicar K-Means con K=8 (seg√∫n el coeficiente de Silhouette)
optimal_k = 8
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df["Family"] = kmeans.fit_predict(X_scaled)

# Convertir las etiquetas en nombres de familia
df["Family"] = df["Family"].apply(lambda x: f"Fam{x}")

# Mostrar cu√°ntos elementos hay en cada familia
print("üìä Distribuci√≥n de malware por familia:")
print(df["Family"].value_counts())


# In[29]:


# Aplicar reducci√≥n de dimensionalidad con t-SNE
tsne = TSNE(random_state=42, perplexity=5.0, n_iter=1000)
tsne_results = tsne.fit_transform(X_scaled)

# Crear DataFrame con los resultados de t-SNE
df_tsne = pd.DataFrame(tsne_results, columns=["TSNE1", "TSNE2"])
df_tsne["Family"] = df["Family"]

# Visualizaci√≥n de clusters con t-SNE
plt.figure(figsize=(8, 6))
sns.set_style("darkgrid", {"grid.color": ".6", "grid.linestyle": ":"})
sns.scatterplot(data=df_tsne, x="TSNE1", y="TSNE2", hue="Family", palette="hls", edgecolor="k", alpha=0.7)
plt.title("Clusters de Malware usando Embeddings de Gemini + t-SNE (Agrupados por Familia)")
plt.xlabel("TSNE1")
plt.ylabel("TSNE2")
plt.legend(title="Familia", bbox_to_anchor=(1, 1))
plt.show()

# Guardar el dataset con embeddings y reducci√≥n de dimensionalidad
df_tsne.to_csv("malware_families_tsne.csv", index=False)
print("‚úÖ Dataset con embeddings y t-SNE guardado como 'malware_families_tsne.csv'")


# In[28]:


# Crear tabla con valores de t-SNE y la familia asignada
df_tsne_display = df_tsne.sample(20)  # Mostrar solo 20 filas aleatorias

# Imprimir tabla en formato bonito
import IPython.display as display
display.display(df_tsne_display)


# # An√°lisis de similitud

# ### Grafos de todo el conjunto

# In[ ]:


import pandas as pd
import networkx as nx
from sklearn.metrics import jaccard_score
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity

# Cargar el archivo CSV
file_path = 'malware_dataset_optimized.csv'
df = pd.read_csv(file_path)

# Mostrar las primeras filas para entender la estructura de los datos
print(df.head())

# Preparar los datos: Convertimos las columnas 'Strings' y 'Num_Imports' a sets para calcular el √≠ndice de Jaccard
df['Strings'] = df['Strings'].apply(lambda x: set(str(x).split(',')))  # Si hay un separador espec√≠fico en 'Strings'
df['Num_Imports'] = df['Num_Imports'].apply(lambda x: set([x]))  # Convertimos el n√∫mero de importaciones a un conjunto


# In[ ]:


# Funci√≥n para calcular el √≠ndice de Jaccard entre dos conjuntos
def jaccard_index(set1, set2):
    return len(set1.intersection(set2)) / len(set1.union(set2))

# Funci√≥n para construir un grafo de similitudes para una familia
def build_graph(df_family, threshold=0.5, feature='Strings'):
    G = nx.Graph()
    
    # Comparar cada par de malwares
    for i, row1 in df_family.iterrows():
        for j, row2 in df_family.iterrows():
            if i >= j:
                continue
            sim = jaccard_index(row1[feature], row2[feature])
            if sim >= threshold:
                G.add_edge(row1['SHA256'], row2['SHA256'], weight=sim)

    return G

# Funci√≥n para construir un grafo para todo el conjunto de malwares
def build_global_graph(df, threshold=0.5, feature='Strings'):
    G = nx.Graph()

    # Comparar cada par de malwares
    for i, row1 in df.iterrows():
        for j, row2 in df.iterrows():
            if i >= j:
                continue
            sim = jaccard_index(row1[feature], row2[feature])
            if sim >= threshold:
                G.add_edge(row1['SHA256'], row2['SHA256'], weight=sim)

    return G


# In[ ]:


def draw_graph_with_layout(graph, title):
    # Usar un layout para distribuir los nodos
    pos = nx.spring_layout(graph, k=0.1, iterations=50)  # `k` controla la distancia entre los nodos
    
    # Dibujar el grafo
    plt.figure(figsize=(12, 8))
    nx.draw(graph, pos, with_labels=True, node_size=200, font_size=8, node_color='lightblue', edge_color='gray')
    plt.title(title)
    plt.show()

# Crear un grafo global
global_graph = build_global_graph(df, threshold=0.5, feature='Strings')

# Visualizar el grafo global original
draw_graph_with_layout(global_graph, "Grafo de similitud global")

# Obtener los componentes conectados
components = list(nx.connected_components(global_graph))

# Dibujar un grafo para cada componente conectado
for idx, component in enumerate(components):
    subgraph = global_graph.subgraph(component)
    draw_graph_with_layout(subgraph, f"Grafo de similitud global - Componente {idx + 1}")


# ### Grafos por familia

# In[ ]:


# Cargar el archivo de malwares y el archivo de familias
file_path_malware = 'malware_dataset_optimized.csv'
file_path_families = 'malware_dataset_families.csv'

# Cargar los datos de los malwares y de las familias
df_malware = pd.read_csv(file_path_malware)
df_families = pd.read_csv(file_path_families)

# Realizar el merge entre los archivos utilizando 'Filename'
df_merged = pd.merge(df_malware, df_families[['Filename', 'Family', 'Embeddings']], on='Filename', how='left')


# In[ ]:


# Funci√≥n para calcular la similitud coseno entre dos vectores de embeddings
def calculate_cosine_similarity(embedding1, embedding2):
    return cosine_similarity([embedding1], [embedding2])[0][0]

# Funci√≥n para construir un grafo de similitudes por familia usando los embeddings
def build_graph_by_family(df_family, threshold=0.8):
    G = nx.Graph()
    
    # Comparar cada par de malwares en la familia
    for i, row1 in df_family.iterrows():
        for j, row2 in df_family.iterrows():
            if i >= j:
                continue
            # Calcular la similitud coseno entre los embeddings
            embedding1 = np.array(eval(row1['Embeddings']))  # Convertir el string de la lista en un array
            embedding2 = np.array(eval(row2['Embeddings']))
            sim = calculate_cosine_similarity(embedding1, embedding2)
            
            # Si la similitud es mayor o igual que el umbral, agregar una arista entre los nodos
            if sim >= threshold:
                G.add_edge(row1['SHA256'], row2['SHA256'], weight=sim)

    return G


# In[ ]:


# Crear un grafo por familia
graphs_by_family = {}
for family in df_merged['Family'].unique():
    df_family = df_merged[df_merged['Family'] == family]
    graphs_by_family[family] = build_graph_by_family(df_family, threshold=0.8)

# Funci√≥n para visualizar los grafos con un layout adecuado
def draw_graph_with_layout(graph, title):
    pos = nx.spring_layout(graph, k=0.1, iterations=50)
    plt.figure(figsize=(12, 8))
    nx.draw(graph, pos, with_labels=True, node_size=200, font_size=8, node_color='lightblue', edge_color='gray')
    plt.title(title)
    plt.show()

# Visualizar los grafos por familia
for family, graph in graphs_by_family.items():
    draw_graph_with_layout(graph, f"Grafo de similitud para la familia {family}")


# Conclusiones
# 
# 1. Para ambos algoritmos, ¬øpara qu√© n√∫mero de cl√∫steres se obtiene el coeficiente de Silhouette m√°s alto?
# 
# Silhouette alcanza su valor m√°s alto en 8 clusters. Tiene un valor cercano a 0.95. 
# 
# 2. Para ambos algoritmos, ¬øEn que medida coincide el coeficiente de Silhouette con el m√©todo del codo?
# 
# 
# 
# 3. Seg√∫n los resultados obtenidos de ambos algoritmos ¬øCu√°ntas familias cree que existen entre los ejemplares de malware proporcionados?
# 
# Al analizar los resultados, existen aproximadamente 5 familias en el malware proporcionado.  
# 
# 4. ¬øEn qu√© medida coincide el an√°lisis de similitud con las familias encontradas utilizando los algoritmos de partici√≥n, para ambas caracter√≠sticas (strings, llamadas a las funciones)?
# 
# El an√°lisis de similitud coincide con las familias encontradas usando el alforitmo de partici√≥n (t-sne). Ambos m√©todos nos dicen que hay 5 familias de malware. 
